{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install llama-index python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "zilliz_uri = os.getenv(\"ZILLIZ_URI\")\n",
    "zilliz_token = os.getenv(\"ZILLIZ_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from llama_index import VectorStoreIndex, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 573/573 [00:00<00:00, 1.20MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 134M/134M [01:26<00:00, 1.55MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 352/352 [00:00<00:00, 2.12MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.21MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 960kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 572kB/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb = MilvusVectorStore(\n",
    "    uri = zilliz_uri,\n",
    "    token = zilliz_token,\n",
    "    collection_name = \"tds_articles\",\n",
    "    similarity_metric = \"L2\",\n",
    "    text_key=\"paragraph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=vdb, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is a large language model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(response='A large language model refers to a type of model that has '\n",
      "                  'the ability to process and understand human language on a '\n",
      "                  'large scale. It is designed to generate coherent and '\n",
      "                  'contextually relevant text based on the input it receives. '\n",
      "                  'These models are trained on vast amounts of text data and '\n",
      "                  'use advanced techniques such as deep learning to learn '\n",
      "                  'patterns and relationships within the language. The '\n",
      "                  'emergence of large language models has been considered a '\n",
      "                  'significant milestone in the field of natural language '\n",
      "                  'processing.',\n",
      "         source_nodes=[NodeWithScore(node=TextNode(id_='92339128-c5cc-417c-a50a-890d062ddb6f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7e3207d34fa042975d8321bf9c2b8ab275ff63968ed46f977eba9cac20cd721b', text='The following papers introduce some milestones on the journey toward large language models.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=24.350723266601562),\n",
      "                       NodeWithScore(node=TextNode(id_='51101b46-569d-4788-9491-18155c6ca0fd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='542b4c95ee63e3332852b04fe2dd640400e34fccb4ca9fad1eb73f6fc6962aa2', text='[7] Jason Wei et al. 2023. Emergent Abilities of Large Language Models.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=24.75406265258789)],\n",
      "         metadata={'51101b46-569d-4788-9491-18155c6ca0fd': {},\n",
      "                   '92339128-c5cc-417c-a50a-890d062ddb6f': {}})\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

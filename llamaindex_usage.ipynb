{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install llama-index python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "zilliz_uri = os.getenv(\"ZILLIZ_URI\")\n",
    "zilliz_token = os.getenv(\"ZILLIZ_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from llama_index import VectorStoreIndex, StorageContext, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 573/573 [00:00<00:00, 1.20MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 134M/134M [01:26<00:00, 1.55MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 352/352 [00:00<00:00, 2.12MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.21MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 960kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 572kB/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb = MilvusVectorStore(\n",
    "    uri = zilliz_uri,\n",
    "    token = zilliz_token,\n",
    "    collection_name = \"tds_articles\",\n",
    "    similarity_metric = \"L2\",\n",
    "    text_key=\"paragraph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = StorageContext.from_defaults(vector_store=vdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=vdb, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='A large language model is a type of artificial intelligence system that is designed to understand and generate human language. It is capable of processing and analyzing vast amounts of text data, allowing it to learn patterns, generate coherent sentences, and even engage in conversations with humans. Large language models have the ability to understand and generate language at a scale that was previously not possible, making them valuable tools for various applications such as natural language processing, machine translation, and text generation.', source_nodes=[NodeWithScore(node=TextNode(id_='00e67676-ce25-4544-a718-749bd770fc4a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7e3207d34fa042975d8321bf9c2b8ab275ff63968ed46f977eba9cac20cd721b', text='The following papers introduce some milestones on the journey toward large language models.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=24.350723266601562), NodeWithScore(node=TextNode(id_='9834f5b1-187c-468d-8d51-c4a329b42e72', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='542b4c95ee63e3332852b04fe2dd640400e34fccb4ca9fad1eb73f6fc6962aa2', text='[7] Jason Wei et al. 2023. Emergent Abilities of Large Language Models.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=24.75406265258789)], metadata={'00e67676-ce25-4544-a718-749bd770fc4a': {}, '9834f5b1-187c-468d-8d51-c4a329b42e72': {}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"What is a large language model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
